---
title: "Homework 1 R markdown"
author: "Abra Brisbin"
date: '`r Sys.Date()`'
output:
  word_document:
    fig_height: 4
    fig_width: 4.5
  pdf_document:
    fig_height: 4
    fig_width: 4.5
  html_document:
    fig_height: 4
    fig_width: 4.5
---

```{r setup, include=FALSE}
#require(mosaic)   # Load additional packages here 

# Some customization.  You can alter or delete as desired (if you know what you are doing).
#trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  echo = TRUE,
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

Load packages here.
```{r, message=FALSE}
library(tidyverse)
```

#### Intellectual Property: 
These problems are the intellectual property of the instructors and may not be reproduced outside of this course.



## Problem 1: Analyzing Gas Mileage  


In this problem, you will use analyze gas mileage using the **Auto** data set from the ISLR library in R.   

#### Question 1 (2 points)

Load the **ISLR** library into R and look at the first few rows of the **Auto** data set.  
```{r}
library(ISLR)
data("Auto")
```

```{r}
glimpse(Auto)
```


What data mining strategy would you use to investigate the following questions?  (Note that the orderings for the scenarios and answer choices on Canvas might differ from those shown below.)

*   You are building an app for a used-car website that will take information about the year, engine displacement, and weight of cars, and determine whether they are most likely American (origin = 1), European (2), or Japanese (3).  

**Multiple Choice Answer (AUTOGRADED on Canvas)**:   


    *Classification*

*   The manager of a used-car lot wants to arrange groups of similar cars on the lot.  The manager wants to understand the relationships between the year, engine displacement, and weight of cars to identify informative groupings.  


**Multiple Choice Answer (AUTOGRADED on Canvas)**:  

    *Unsupervised learning*

*   You are building an app for a used-car website that will take information about the year, engine displacement, and weight of cars, and estimate their horsepower.  


**Multiple Choice Answer (AUTOGRADED on Canvas)**:

     *Regression*



#### Question 2 (3 points)

We would like to use K-nearest neighbors to predict the gas mileage (`mpg`, miles per gallon) of cars based on their weight (in pounds) and their year of manufacture.  Fill in the blanks:  In this analysis, the main reason why standardizing the data is a (good/bad) idea is because the variables (mpg and weight, mpg and year, weight and year) have very (similar/different) (means/standard deviations). 

**Fill-in-the-blank Answer**: 
 
In this analysis, the main reason why standardizing the data is a good idea is because the variables (mpg and weight) have very different standard deviations. 


#### Question 3 (2 points)
Set R's seed to 1 (for Homework 1) with:
**set.seed(1)**
```{r}
set.seed(111)
```

Create a `groups` vector of 256 copies of the number 1 (to represent observations that will be in the training set) and 136 copies of the number 2 (to represent observations that will be in the validation set.  Then use **sample()** to randomize the order of the vector.

Make a vector that contains TRUE for each data point of `Auto` that will be in the training set, and FALSE for each data point that will be in the test (or validation) set.

Enter your R code below.

```{r}
set.seed(100)

# create a vector with 256 copies of 1 (training) and 136 copies of 2 (test)
# "n copies of x" is a hint you need to use `rep`
# `c` concatenates the two parts into a single vector 
groups <- c(rep(1, 256), rep(2, 136))

# use sample to randomise the order of the vector
groups <- sample(groups, replace = FALSE)

# make a vector that contains TRUE for each data point in the training set.
# Values of `1` are training data
# `groups == 1` creates a logical vector
train_set <- groups == 1

train_data <- Auto[train_set, ]
valid_data <- Auto[!train_set, ]
```


**Code Answer**: 
```{r}
set.seed(100)

indexes <- sample(nrow(Auto), ( 0.655*nrow(Auto)), replace = FALSE)
#indexes <- sample(nrow(Auto), ( 256/392*nrow(Auto)), replace = FALSE)


number1 <- c(Auto[indexes, ])
number2 <- c(Auto[-indexes, ])

#newdata <- setdiff(number1, number2)

#train_data  <- Auto[indexes, ]
#valid_data <- Auto[-indexes, ]


```

```{r}
contrain_vec <- Auto %>%
  rowwise %>%
  mutate(contain_train = any(unlist(name) %in% number1$name)) %>%
  select(contain_train)
  
```


#### Question 4 (2 points)
Standardize the `weight` and `year` columns of the training set.  Then standardize the `weight` and `year` columns of the test set, *using the original mean and standard deviation of the training set*.

Enter your R code below.  
**Code Answer**: 
```{r}

stand_train <- train_data %>% mutate_at(c("weight", "year"), ~(scale(.) %>% as.vector))

stand_valid <- valid_data %>% mutate_at(c("weight", "year"), ~(scale(.) %>% as.vector))



```



#### Question 5: **(3 points)**

Use 1-nearest neighbor regression (fit on the standardized training data) to predict the gas mileage of the cars in the standardized validation set.  Compute the mean squared error.  

Enter your R code below.  

**Code Answer**: 
```{r}
library(class) 
# k = 1, i.e only one cluster
stand_train  <- na.omit(stand_train)
stand_valid  <-  na.omit(stand_valid)
knn_model1 <- knn(stand_train[2:8],
                 stand_valid[2:8],
                 cl=stand_train$mpg,
                 k=1)

#knn_model1
```




```{r}
library(FNN)
```







#### Question 6 (1 point)

What is the MSE for the validation set?  (Round your answer to 2 decimal places.)

Your Answer:  
**Numeric Answer (AUTOGRADED on Canvas)**:  


```{r}
knn1 <- knn.reg(train=stand_train[2:8],   
                     y=stand_train$mpg,
                     test= stand_valid[2:8],
                     k=1) 

mse  <- mean((knn1$pred -stand_valid$mpg)^2)  


cat("mse : ", mse, sep='')


```


```{r}
test.features = subset(valid_data, select=-c(mpg))
test.target = subset(valid_data, select=mpg)[,1]


```



#### Question 7 (4 points)**

Use a for() loop to apply K-nearest neighbors regression to the same training and validation sets, for values of k from 1 to 50.  Make a plot of the MSE (calculated for the validation set predictions) as a function of k.  

Enter your R code and plot on this question on Canvas.  (Use **Insert** -> **Image** to insert the plot.)  
**Code and Graph Answer**: 


```{r}
train_mse <- c()
test.target = subset(valid_data, select=mpg)[,1]
for (i in 1:50){
  set.seed(1)
  train_pred <- knn.reg(train=stand_train[2:8], y=stand_train$mpg, test= stand_valid[2:8],k=i) 
  train_mse <- c(train_mse, sqrt(mean((test.target - train_pred$pred)^2)) )
}

pdf("Answer4.pdf") 
plot(1:50, train_mse)

lines(1:50, train_mse, lwd=2, col='cornflowerblue')
dev.off()
```





#### Question 8: **(2 points)**

In your opinion, which value of k is the best choice?  Why?

**Text Answer**: 

Choosing the optimal value for K is best done by first inspecting the data. In general, a large K value is more precise as it reduces the overall noise; however, the compromise is that the distinct boundaries within the feature space are blurred. Cross-validation is another way to retrospectively determine a good K value by using an independent data set to validate your K value. 


The best model k 4  for which the minimum MSE =3.377841 




## Problem 2:  

In this problem, you will use K-nearest neighbors to classify peopleâ€™s income as >\$50,000 or \$50,000.

You are about to start **Problem 2 of 2**, which analyzes personal income using the Census_income.csv data file (available under Canvas Lesson 1 resources).   You can find more information at [the data source](https://archive.ics.uci.edu/ml/datasets/census+income).  

Data Source:  Kohavi, R and B. Becker. (1996). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.  




#### Question 9: (2 points)

Read the data into R.  One-hot encode the variable `Sex`, using `Male` as the default value. (Unfortunately, because this data set is from the US Census, it did not allow allow an option of Intersex, or distinguish between sex and gender.)

Enter your R code below.  
**Code Answer**: 

```{r}
data  <- read_csv("/Users/garbamoussa/Downloads/B Census_income.csv")
```

```{r}
glimpse(data)
```


#### Question 10 (2 points)

Set R's random seed to 1 again.  Then randomly sample 20,000 individuals to be in the training set.

Enter your R code below.  
**Code Answer**: 
```{r}

sample_size = floor(0.614232*nrow(data))
sample_size
set.seed(111)

train_ind = sample(seq_len(nrow(data)),size = sample_size)  
training =data[train_ind,] 
testing=data[-train_ind,]
```



#### Question 11 (2 points)
Standardize the EducYears and Age variables from the training data.  Combine these with the unstandardized, one-hot encoded Sex variable from the training data to create a data frame `x_train`.

Use the original means and standard deviations from the training data to standardize EducYears and Age from the validation data.  Combine these with the unstandardized, one-hot encoded Sex variable to create a data frame `x_test`. 

Enter your R code below.  

**Code Answer**: 
```{r}
#training$EducYears = scale(training$EducYears, center= TRUE, scale=TRUE)
#training$Age = scale(training$Age, center= TRUE, scale=TRUE)
```
 
 
```{r}
library(mltools)
```


```{r}
training <- training %>% mutate_at(c("EducYears", "Age"), ~(scale(.) %>% as.vector))
x_train <- training[c("EducYears","Age","Sex")]
x_train <- mutate(x_train, Sex = ifelse(Sex=="Male",1,0))

```


```{r}
library(timetk)
```

```{r}
EducYears_std <- standardize_vec(testing$EducYears)

```


```{r}
Age_std <- standardize_vec(training$Age)
```


```{r}
testing$Age    <- standardize_inv_vec(standardize_vec(testing$Age),
                                 mean =  -2.38886665508112e-16,
                                 sd   = 1)

```

 
```{r}
x_test <- testing[c("EducYears","Age","Sex")]
x_test <- mutate(x_test, Sex = ifelse(Sex=="Male",1,0))
```

 
#### Question 12 (2 points)

Use 25-nearest neighbor classification (fit on the training set) to predict whether the income of each individual in the validation set is >50K or <=50K. 

Find the confusion matrix.  You should be able to produce a matrix table with two rows and two columns, similar to the one below. 

Please enter the information as whole numbers.  Note carefully the labels for the rows and columns, and be sure to orient your table accordingly.

```{r}
unique(data$Income)
```


```{r}

library(class)
training$Income <- as.factor(training$Income)
mod1 <- knn(x_train, x_test, training$Income, k=25, prob=TRUE)
prob <- attr(mod1, "prob")

prob <- ifelse( mod1=="1", prob, 1-prob)

cm <- table(testing$Income,mod1)

cm 
```

Please enter the information *exactly as it appears in R*.

.                 | Actual income <= 50K | Actual Income > 50K
----------------- | -------------------- | -------------------
Classified <= 50K	| **[A]** | **[B]** | 
Classified > 50K	| **[C]** | **[D]** | 
	

**Numeric Answer (AUTOGRADED on Canvas)**:  
[A] =   8617 
[B] =  891
[C] =  1739
[D] =  1314




#### Question 13 (1 point)

What is the overall error rate on the validation set? Enter your answer as a decimal between 0 and 1, rounded to 4 decimal places.

```{r}
accu0 <- length(which(training$Income==mod1)==TRUE)/length(training$Income)
sens0 <- length(which((training$Income==mod1) & (training$Income=="<=50K"))) / length(which(training$Income=="<=50K"))
spec0 <- length(which((training$Income==mod1) & (training$Income==">50K"))) / length(which(training$Income==">50K"))
cat("Accuracy=",round(accu0,4),'\n',"Sensitivity=",round(sens0,4),'\n',"Specificity=",round(spec0,4))

```
```{r}
#computing error rate against test data set
s70_30 <- 1-((cm[1] +cm[2,2])/(sum(cm)))

cat("The error rate for 20000 Training vs 12561 testing split is: ", s70_30)

```

**Numeric Answer (AUTOGRADED on Canvas)**:


```{r}
# misclassification rate  
mean(mod1!=testing$Income) * 100
```
```{r}
1-sum(diag(cm))/sum(cm)
```


#### Question 14 (1 point)

What proportion of people making > $50,000 were misclassified? Enter your answer as a decimal between 0 and 1, rounded to 4 decimal places.

```{r}

# false negative rate
round(891/(891+1314), 4) 

```


**Numeric Answer(AUTOGRADED on Canvas)**:

  0.4041
 
#### Question 15 (2 points)
Make a grid of example points with values of education from 1 to 16, ages from 17 to 75, and sex from 0 to 1.  Standardize education and age using the original mean and standard deviation of the training set (from question 11).

Create a data frame, `x_example`, containing the standardized education and age and the unstandardized sex from the example points.  The order of the columns should match the order of the columns in `x_train`.

Enter your R code below.

```{r}
unique(data$Education)
```

```{r}
x_example <- crossing(Education = 1:16,
         Age = 17:75,
         Sex = c(0,1))

x_example
```

```{r}
x_example$Age    <- standardize_inv_vec(standardize_vec(x_example$Age),
                                 mean =  -2.38886665508112e-16,
                                 sd   = 1)
```


```{r}
training$Education <- 
  ifelse(training$Education == "Bachelors", 1,
         ifelse(training$Education == "HS-grad", 2,
          ifelse(training$Education == "11th", 3 ,
            ifelse(training$Education == "Masters", 4,
              ifelse(training$Education == "9th", 5 ,
                ifelse(training$Education == "Some-college", 6 ,
                       ifelse(training$Education == "Assoc-acdm", 7,
                              ifelse(training$Education == "Assoc-voc",  8,
                  ifelse(training$Education ==  "7th-8th", 9,   
                  ifelse(training$Education == "Doctorate" , 10,
                         
                  ifelse(training$Education == "Prof-school", 11, 
                         
                    ifelse(training$Education == "5th-6th", 12, 
                           ifelse(training$Education == "10th", 13, 
                                  ifelse(training$Education =="1st-4th",14 , 
                                         ifelse(training$Education == "Preschool", 15, 
                         ifelse(training$Education == "12th", 16, 
                                
                         NA))))))) )))))))))

```




```{r}
Education_std <- standardize_vec(training$Education)
```


```{r}
x_example$Age    <- standardize_inv_vec(standardize_vec(x_example$Age),
                                 mean =  4.4242,
                                 sd   = 3.44385646730063)
```




```{r}
unique(data$Age)
```


**Code Answer**: 
```{r}
unique(training$Age)
```



```{r}
training <- mutate(training, Sex = ifelse(Sex=="Male",1,0))
```

```{r}
glimpse(training)
```

```{r}
x_example <- training[c("Education","Age","Sex")]
```

```{r}
#x_example <- x_example %>% mutate_at(c("Education", "Age"), ~(scale(.) %>% as.vector))
```


#### Question 16 (2 points)
Use 25-nearest neighbors to predict the income classifications of the example points, using the same training data as in question 12.  Make graphs showing the relationship between education, age, sex, and predicted income.

Use **Insert** -> **Image** to upload your graphs to this question on Canvas. 

**Graph Answer**:



```{r}
training$Income <- as.factor(training$Income)
predicted_Income <- knn(x_train, x_example, training$Income, k=25, prob=TRUE)

prob <- attr(predicted_Income, "prob")
prob <- ifelse( predicted_Income=="1", prob, 1-prob)
```


```{r}
predictions <- cbind(data.frame(training$Income, predicted_Income))
summary(predictions)
```


```{r}
result <- cbind(x_example, predicted_Income)
combinetest <- cbind(x_example, training$Income)

#pdf("answer16.pdf") 
result%>%
    ggplot(aes(x=Education, y=Age, color=predicted_Income))+
    geom_point(size=3)

#dev.off()

```

```{r}

ggplot(result, aes(x = Age, y = Education, color=predicted_Income)) +
  geom_point() +
  facet_wrap(~ Sex)

```


```{r}
library(gridExtra)
```

```{r}
pdf("answer16.pdf") 
g1 <- ggplot(result, aes(x = predicted_Income, y = Age, col = predicted_Income)) + 
  geom_jitter() + 
  theme(legend.position = "none") + 
  ggtitle("Age vs predicted_Income - Jitter Plot") 

g2 <- ggplot(result, aes(x = predicted_Income, y = Education, fill = predicted_Income)) + 
  geom_boxplot() + 
  theme(legend.position = "none") + 
  ggtitle("Education vs predicted_Income - Boxplot")

g3 <- ggplot(result, aes(x = predicted_Income, y = Sex, fill = predicted_Income)) + 
  geom_boxplot() + 
  theme(legend.position = "none") + 
  ggtitle("Sex vs predicted_Income - Boxplot")

grid.arrange(g1, g2, g3, 
             ncol = 2, 
             nrow = 2)

dev.off()
```

#### Question 17 (3 points)
Write 3-6 sentences interpreting the graphs you made in the previous question.  (For purposes of interpreting the results, note that the data are from the 1990s.)

**Text Answer**: 

power between different graphs (e.g. scatter plots & box plots), but if I had to guess I would say that Age & Education are the two  strongest predictors of Income.

Age looks like it might be a little weaker, with year & acceleration weaker still (more significant overlap of the distributions).

It is harder to judge how strong a predictor origin is visually when compared with the other variables, since we are looking at a relationship between two nominal categorical variables, but it certainly appears to be on the weaker side.



