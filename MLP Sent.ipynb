{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e10d303",
   "metadata": {},
   "source": [
    "# 1. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "de599feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703c451",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6a0298a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(\"train_data.npy\")\n",
    "train_y = np.load(\"train_label.npy\")\n",
    "\n",
    "test_x = np.load(\"test_data.npy\")\n",
    "test_y = np.load(\"test_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "aa72f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(50000, 1)\n",
      "(10000, 128)\n",
      "[[3]\n",
      " [8]\n",
      " [8]\n",
      " ...\n",
      " [5]\n",
      " [1]\n",
      " [7]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75791348",
   "metadata": {},
   "source": [
    "# 3. Data Pre-processing\n",
    "\n",
    "## 3.1. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "712ef482",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_norm = (train_x - np.min(train_x))/(np.max(train_x) - np.min(train_x))\n",
    "train_y_norm = (train_y - np.min(train_y))/(np.max(train_y) - np.min(train_y))\n",
    "\n",
    "test_x_norm = (test_x - np.min(test_x))/(np.max(test_x) - np.min(test_x))\n",
    "test_y_norm = (test_y - np.min(test_y))/(np.max(test_y) - np.min(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b18be5",
   "metadata": {},
   "source": [
    "## 3.2. One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7b71e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.unique(train_y_norm).size\n",
    "        \n",
    "encoded_train_y = []\n",
    "\n",
    "for label in train_y_norm:\n",
    "    encoded_label = np.zeros(num_classes)\n",
    "    encoded_label[int(label)] = 1\n",
    "    encoded_train_y.append(encoded_label)\n",
    "\n",
    "encoded_train_y = np.array(encoded_train_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e445c1b",
   "metadata": {},
   "source": [
    "# 4. Main Classes\n",
    "\n",
    "## 4.1. Activation Functions\n",
    "\n",
    "Defines activation functions for the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "823e0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  a * (1 - a )\n",
    "\n",
    "    def __relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "  \n",
    "    def __relu_deriv(self,a):\n",
    "        return np.heaviside(a, 0)\n",
    "\n",
    "    def __softmax(self, z):\n",
    "        z = np.atleast_2d(z)\n",
    "        max_z = np.max(z, axis=1)\n",
    "        z = [z[i] - max_z[i] for i in range(max_z.shape[0])] # Numerical Stability\n",
    "        z = np.array(z)\n",
    "        return np.divide(np.exp(z).T, np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def __softmax_deriv(self, y, y_hat):\n",
    "        return y_hat - y\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'relu': \n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "        elif activation == \"softmax\":\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729888c7",
   "metadata": {},
   "source": [
    "## 4.2. Hidden Layers\n",
    "\n",
    "Defines a typical hidden layer in the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "cffbb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):    \n",
    "    def __init__(self,\n",
    "                 n_in, \n",
    "                 n_out,\n",
    "                 activation_last_layer='relu',\n",
    "                 activation='relu',\n",
    "                 W=None, \n",
    "                 b=None,\n",
    "                 output_layer = False,\n",
    "                 v_W = None,\n",
    "                 v_b = None):\n",
    "        \"\"\"\n",
    "        A class that defines a typical hidden layer in the MLP: \n",
    "        units are fully-connected and have relu activation function. \n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        Weight matrix W is of shape (n_in,n_out) i.e. (no. of neurons in previous layer, no. of neurons in next layer)\n",
    "        so when input matrix * weight matrix = (batch size, no. of neurons in next layer)\n",
    "        and the bias vector b is of shape (n_out,). i.e., the no. of neurons in next layer as we add bias AFTER weight multiplication\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        n_in (int):                         dimensionality of the input into this layer \n",
    "        n_out (int):                        dimensionality of the output of this layer, i.e. the number of hidden units/neurons in this layer\n",
    "        activation (str):                   Activation of the current hidden layer\n",
    "        activation_last_layer (str):        Activation of the previous layer\n",
    "        W (array):                          the weight matrix that is multiplied to the input of this layer, shape is (n_in,n_out) \n",
    "        b (array):                          the bias matrix that is added to the input of this layer, shape is (n_out, )\n",
    "        output_layer (boolean):       a boolean to determine if this layer is the final layer of the MLP\n",
    "        v_W (array):                        v(t) for the weights used in momentum SGD\n",
    "        v_b (array):                        v(t) for the bias terms used in momentum SGD\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set class attributes\n",
    "        self.input=None\n",
    "        self.activation = Activation(activation).f\n",
    "        self.activation_deriv_last_layer= None\n",
    "        self.output_layer = output_layer\n",
    "        \n",
    "        if activation_last_layer is not None:\n",
    "            self.activation_deriv_last_layer=Activation(activation_last_layer).f_deriv # derivative activation for the previous layer in forwards direction; used for backpropagation function below\n",
    "\n",
    "        # randomly assign small values for the weights as the initiallization\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "\n",
    "        # set the size of bias as the size of output dimension\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        # set the size of weight gradation as the size of weight\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        self.v_W = np.zeros_like(self.grad_W)\n",
    "        self.v_b = np.zeros_like(self.grad_b)\n",
    "        \n",
    "        # an array with the neurons to be dropped for the current layer (initalized as zeroes but will be changed later on)\n",
    "        self.dropout_array=np.zeros(n_out)\n",
    "        \n",
    "    \n",
    "    def forward(self, X, prop_dropout):\n",
    "        '''\n",
    "        Feedsforward a single hidden layer in the MLP in the training process\n",
    "        Includes functionality for dropout as a regularization tool\n",
    "\n",
    "        Parameters:\n",
    "        X (array): The input data (either the initial data or the output from the previous layer)\n",
    "        prop_dropout (float): the probability of neurons dropped in the feed-forward process\n",
    "        \n",
    "        Returns:\n",
    "        output: the output of the current layer in MLP\n",
    "        '''\n",
    "        \n",
    "        # Computes the \n",
    "        lin_output = np.dot(X, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output))\n",
    "        \n",
    "        # Dropout: training the network with only a subset of the neurons\n",
    "        if not self.output_layer:\n",
    "            self.dropout_array = np.random.binomial(1, prop_dropout, size = self.output.shape) # creates random array of 0 and 1's, with 0's representing neurons to drop\n",
    "            self.output = self.dropout_array * self.output\n",
    "\n",
    "        self.input=X\n",
    "        return self.output\n",
    "    \n",
    "\n",
    "    def forwardtest(self, X, p_dropout):\n",
    "        '''\n",
    "        Feedsforward a single hidden layer in the MLP in the test process\n",
    "        Unlike in fowardpropagation in training, dropout out here is implemented as output scaled down by p_dropout\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): The input data (either the initial data or the output from the previous layer)\n",
    "        prop_dropout (float): the probability that outputs will be scaled down in feed-forward process\n",
    "        '''\n",
    "        \n",
    "        lin_output = np.dot(X, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        \n",
    "        # Dropout: scales the outputs of each hidden layer down by prop_dropout during testing\n",
    "        if not self.output_layer:\n",
    "            self.output *= p_dropout\n",
    "        \n",
    "        self.input=X\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, delta, layer_num, output_layer=False):         \n",
    "        \n",
    "        \"\"\"\n",
    "        Backprogagates a single hidden layer:\n",
    "        1. Calculates the gradient for this current layer to be used in the update function in the MLP class\n",
    "        2. Calculates the delta for this current hidden layer and feeds it back to the MLP.backward() function\n",
    "        3. Includes functionality for dropout for the delta\n",
    "\n",
    "        Parameters:\n",
    "        delta (array): delta value of current layer, retrieved from the MLP.backward() and used to calculate the gradient of this layer\n",
    "        layer_num (int): index of current layer number retrieved from MLP.backward(); index starts from 0\n",
    "        output_layer (boolean): boolean to determine if the current layer is not the output layer in the MLP\n",
    "        \n",
    "        Returns:\n",
    "        delta (array): delta value of below layer for only the neurons that were not killed off in the forward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        # gradient for this layer\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.average(delta, axis=0)\n",
    "        \n",
    "        # delta for below layer using delta of current layer and derivative of activation of below layer\n",
    "        if self.activation_deriv_last_layer: # checks if below layer is not input layer\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv_last_layer(self.input)\n",
    "\n",
    "        # Dropout: back propagates the gradients through the neurons that were not killed off during the forward pass\n",
    "        if (layer_num != 0) and (not self.output_layer):  # layer_num != 0 because the next layer below is input layer\n",
    "            delta = delta * nn.layers[layer_num-1].dropout_array # layer_num-1 because we want to use the dropout array for the below layer\n",
    "\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15161d8a",
   "metadata": {},
   "source": [
    "## 4.3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3e6877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    " \n",
    "    def __init__(self, layers_neuron_count, activation=[None,'relu', 'relu','relu','softmax'], weight_decay = 0.98):\n",
    "        \"\"\"\n",
    "        Initializes the MLP:\n",
    "        The code will create all layers automatically based on the provided parameters.\n",
    "\n",
    "        Parameters:\n",
    "        layers_neuron_count (list of int):  A list containing the number of units/neurons in each layer.\n",
    "        activation (list of str):           A list containing the activation function to be used in each layer. Can be \"logistic\", \"tanh\", \"relu\", or \"softmax\"\n",
    "        weight_decay (float):               The value to decay the weight in the loss function for regularization\n",
    "        \"\"\"        \n",
    "\n",
    "        # Set class attributes\n",
    "        self.layers=[] # A list of every layer in the MLP\n",
    "        self.params=[]\n",
    "        self.activation = activation\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Add hidden layers to the MLP one by one\n",
    "        for i in range(len(layers_neuron_count)-1):\n",
    "            \n",
    "            output_layer = False # Flag to signal if the current layer in the loop is final layer\n",
    "        \n",
    "            if i == len(layers_neuron_count) -2: # Check if output layer, -1 for input layer, -1 since index is 0\n",
    "                output_layer = True\n",
    "            \n",
    "            self.layers.append(HiddenLayer(layers_neuron_count[i],\n",
    "                                         layers_neuron_count[i+1],\n",
    "                                         activation_last_layer=activation[i],\n",
    "                                         activation=activation[i+1],\n",
    "                                         output_layer = output_layer))\n",
    "\n",
    "    def forward(self, X, prop_dropout):\n",
    "        \"\"\"\n",
    "        Feedsforward the input data sequentially through each layer in the MLP in training process:\n",
    "        In this function, a loop, each layer of the MLP takes the output from the previous layer as input (except the input layer, where it would take the initial data as input) \n",
    "        and then performs a calculation on this input to create the output for this layer. \n",
    "        This output is then fed as input into the subsequent layer and the calculation is repeated again.\n",
    "        The exact calculation performed on the input is done throught the HiddenLayer class' forward() method.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): The initial data that is used as input for the first/input layer in the feedforward process.\n",
    "        prop_dropout (float): the probability of neurons to be dropped in the feed-forward process\n",
    "        \n",
    "        Returns:\n",
    "        output (array): The final output from the feedforward process after the initial data has passed through all layers in the MLP.\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(X, prop_dropout)\n",
    "            X=output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forwardtest(self, X, prop_dropout):\n",
    "        \"\"\"\n",
    "        Feedsforward the input data sequentially through each layer in the MLP in testing:\n",
    "        In this function, a loop, each layer of the MLP takes the output from the previous layer as input (except the input layer, where it would take the initial data as input) \n",
    "        and then performs a calculation on this input to create the output for this layer. \n",
    "        This output is then fed as input into the subsequent layer and the calculation is repeated again.\n",
    "        The exact calculation performed on the input is done throught the HiddenLayer class' forward() method.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): The initial data that is used as input for the first/input layer in the feedforward process.\n",
    "        prop_dropout (float): the probability that outputs will be scaled down in feed-forward process\n",
    "        \n",
    "        Returns:\n",
    "        output (array): The final output from the feedforward process after the initial data has passed through all layers in the MLP.\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output=layer.forwardtest(X=X, p_dropout=prop_dropout)\n",
    "            X=output\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def CE_loss(self,y,y_hat):\n",
    "        \"\"\"\n",
    "        1. Calculates the Cross-Entropy( CE) loss \n",
    "        Computes the CE loss of all the samples fed through the MLP in the feedforward process, and then computes their average CE loss as we are\n",
    "        implementing mini-batch training.\n",
    "        2. Calculates delta of the output layer (which as to have Softmax as activation function) in the MLP so it can be used in the backpropagation process\n",
    "        \n",
    "        Parameters:\n",
    "        y (array):  the actual y values from the dataset\n",
    "        y_hat ():   the predicted y values using the MLP feedforward process\n",
    "        \n",
    "        Returns:\n",
    "        loss (float):   The average cross-entropy loss for all samples in the mini-batch fed through the MLP in the feedforward process\n",
    "        output_layer_deltafinal_layer_delta (array):  The delta of the output layer (Softmax activation layer) to be used in the backpropagation process\n",
    "        \"\"\"\n",
    "        \n",
    "        # CE loss calculation\n",
    "#         exps = np.exp(y_hat - np.max(y_hat, axis = 1, keepdims = True))\n",
    "#         last_y_out = exps / np.sum(exps, axis = 1, keepdims = True)\n",
    "#         error = (last_y_out - y)/ y.shape[0]\n",
    "#         loss = (-1*np.nansum(y * np.log(y_hat)))/ y.shape[0]\n",
    "        loss = - np.nansum(y * np.log(y_hat))\n",
    "        loss = loss / y.shape[0] \n",
    "        loss = loss*self.weight_decay\n",
    "\n",
    "        # Delta of output layer calcualtion\n",
    "        # Output_layer_delta = error * Activation(self.activation[-1]).f_deriv(y, y_hat)\n",
    "        output_layer_delta = Activation(self.activation[-1]).f_deriv(y, y_hat)\n",
    "        \n",
    "        return loss, output_layer_delta\n",
    "  \n",
    "    def backward(self, output_layer_delta):\n",
    "        \"\"\"\n",
    "        The backpropagation process conducted backwards across every layer, starting from the output layer of the MLP:\n",
    "        Iteratively updates delta for each layer by using the previous layers delta as input to the HiddenLayer.backward() method\n",
    "        \n",
    "        Parameters:\n",
    "        output_layer_delta (array): The calcualted delta for the output layer retrieved from the CE loss function\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: backward propagates the output layer of the MLP using output_layer_delta to:\n",
    "            # 1: find and adds the gradient of output layer as an attribute\n",
    "            # 2. returns the delta for the final hidden layer of the MLP\n",
    "        delta = self.layers[-1].backward(output_layer_delta, len(self.layers) -1, output_layer = True) # returns final hidden layer delta\n",
    "        \n",
    "        # Step 2: backward propagates the remaining hidden layers (starting from the final hidden layer) through a loop\n",
    "        # in each iteration:\n",
    "            # 1. finds and adds the gradient for that hidden layer as an attribute\n",
    "            # 2. and returns the delta for the next layer below in the MLP which will then be iteratively used as input for calculating the gradient for that respective layer\n",
    "        for layer_num, layer in reversed(list(enumerate(self.layers[:-1]))):\n",
    "            delta = layer.backward(delta, layer_num)\n",
    "\n",
    "    \n",
    "    def update(self,lr,optimization):\n",
    "        \"\"\"\n",
    "        NOTE: run this function AFTER MLP.backward() has been run\n",
    "        Updates the weights and bias of every layer in the MLP according to Stochastic Gradient Descent (SGD) (i.e. according to the delta's calculated in the backward process)\n",
    "        Includes functionality to optimize SGD via Momentum and regularize the model via Weight decay \n",
    "        \n",
    "        Parameters:\n",
    "        lr (float):                         Learning rate for the parameter updates\n",
    "        optimization (dict of str: float):  A dictionary with the key as the type of optimization (functionality will be provided only for 1 key: \"Momentum\"), and values of the key are the parameter values for that optimization method\n",
    "        \"\"\"\n",
    "        \n",
    "        if optimization is None:\n",
    "            for layer in self.layers:\n",
    "                layer.W -= lr * layer.grad_W\n",
    "                layer.b -= lr * layer.grad_b\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                layer.v_W = (optimization['Momentum'] * layer.v_W) + (lr * layer.grad_W)\n",
    "                layer.v_b = (optimization['Momentum'] * layer.v_b) + (lr * layer.grad_b)\n",
    "                layer.W = (layer.W - layer.v_W)*self.weight_decay\n",
    "                layer.b = layer.b - layer.v_b\n",
    "\n",
    "  \n",
    "    # it will return all losses within the whole training process.\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=100, optimization = {\"Momentum\":0.9}, batch_size = 1, prop_dropout=None):\n",
    "        \"\"\"\n",
    "        Trains the MLP with mini-batches:\n",
    "        Iterates through epochs; In each epoch, the entire dataset is trained in batches, with each batch being fed forward to learn the loss (and delta)\n",
    "        and then backpropagated to update the weights and bias of the MLP\n",
    "        \n",
    "        Parameters: \n",
    "        X (array):                          Input data or features\n",
    "        y (array):                          Input targets\n",
    "        learning_rate (float):              parameters defining the speed of learning\n",
    "        epochs (int):                       number of times the dataset is presented to the network for learning\n",
    "        optimization (dict of str: float):  A dictionary with the key as the type of optimization (functionality will be provided only for 1 key: \"Momentum\"), and values of the key are the parameter values for that optimization method\n",
    "        batch_size (int):                   the number of samples in each batch to be used in mini-batch leraning\n",
    "        prop_dropout (float):               probability of dropout \n",
    "        \n",
    "        Returns:\n",
    "        to_return (list):                   A list of Cross Entropy loss for each epoch\n",
    "        \"\"\" \n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        \n",
    "        # no. of batches in an epoch/no. of iterations in an epoch\n",
    "        num_batches = int(np.ceil(X.shape[0]/batch_size)) \n",
    "        \n",
    "        to_return = np.zeros(epochs)\n",
    "        training_accuracy = []\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            loss = np.zeros(num_batches) \n",
    "            current_index = 0 \n",
    "\n",
    "            #Shuffle the data to ensure that each epoch will have different sequence of observations\n",
    "            shuffled_index = np.arange(X.shape[0])\n",
    "            np.random.shuffle(shuffled_index)\n",
    "            X = X[shuffled_index]\n",
    "            y = y[shuffled_index]\n",
    "\n",
    "            for batch_index in range(num_batches):\n",
    "                #forward pass \n",
    "                y_hat = self.forward(X[current_index : current_index + batch_size, ],prop_dropout=prop_dropout)\n",
    "\n",
    "                #backward pass\n",
    "                loss[batch_index], delta = self.CE_loss(y[current_index : current_index + batch_size], y_hat)\n",
    "                self.backward(delta)\n",
    "\n",
    "                #update\n",
    "                self.update(learning_rate, optimization)\n",
    "\n",
    "                #Update the index based on the batch window for the next batch in mini-batch learning\n",
    "                if (current_index + batch_size) > X.shape[0]:\n",
    "                    batch_size = X.shape[0] - current_index\n",
    "                current_index += batch_size\n",
    "            \n",
    "            to_return[k] = np.mean(loss)\n",
    "            \n",
    "            # training loss for this epoch\n",
    "            train_predict = self.predict(x=X, prop_dropout=prop_dropout)\n",
    "            ## Decode one-hot encoded labels back to class labels by finding index of maxmax value in the one-hot encoded array\n",
    "            decoded_predictions = np.zeros(train_predict.shape[0])\n",
    "            for prediction_idx, prediction_vector in enumerate(train_predict):\n",
    "                decoded_predictions[prediction_idx] = int(np.argmax(prediction_vector)) \n",
    "            train_predict = decoded_predictions\n",
    "            #train_accuracy = np.sum(train_predict ==  test_y_norm[:, 0]) / train_predict.shape[0]\n",
    "            #training_accuracy.append(train_accuracy)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"Epoch {}/{} | Loss {:.7f} | used {:.2f} seconds\".format(str(k+1), str(epochs), to_return[k],epoch_time))\n",
    "            print(\"------------------------------------------------------------\")\n",
    "        \n",
    "        return to_return\n",
    "\n",
    "    \n",
    "    def predict(self, x, prop_dropout):\n",
    "        \"\"\"\n",
    "        Predict results of new data in testing using an already trained network\n",
    "        \n",
    "        Parameters:\n",
    "        x (array): input x data in which the MLP will use to make predictions\n",
    "        \n",
    "        Returns:\n",
    "        predictions (array): the predictions from test data using the trained network\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        predictions = [i for i in range(x.shape[0])]\n",
    "        \n",
    "        # feeds forward each sample in test set through the MLP to retrieve prediction\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            predictions[i] = self.forwardtest(X=x[i,] , prop_dropout= prop_dropout)\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3b645",
   "metadata": {},
   "source": [
    "# 5. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "69227862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "LAYERS_NEURON_COUNT = [train_x_norm.shape[1],120,100,50,10]\n",
    "LAYER_ACTIVATION_FUNCS = [None,'relu', 'relu','relu','softmax']\n",
    "WEIGHT_DECAY = 0.98\n",
    "\n",
    "# Fitting\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "OPTIM = {\"Momentum\":0.9} # or None\n",
    "BATCH_SIZE = 100\n",
    "DROPOUT_PROB = 0.5 # or None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f1de3",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "0fd309c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MLP\n",
    "\n",
    "nn = MLP(layers_neuron_count = LAYERS_NEURON_COUNT, \n",
    "         activation=LAYER_ACTIVATION_FUNCS,\n",
    "         weight_decay = WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d73cca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss 0.4089659 | used 3.16 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10 | Loss 0.3447671 | used 3.03 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10 | Loss 0.3416325 | used 3.00 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10 | Loss 0.3412093 | used 3.18 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10 | Loss 0.3424224 | used 3.04 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10 | Loss 0.3387690 | used 2.98 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10 | Loss 0.3384406 | used 3.06 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10 | Loss 0.3370427 | used 2.97 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10 | Loss 0.3364465 | used 3.11 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10 | Loss 0.3361644 | used 2.97 seconds\n",
      "------------------------------------------------------------\n",
      "TIME USED TO TRAIN ENTIRE DATASET IN 10 EPOCHS:\n",
      "0 minute 30.52 seconds\n"
     ]
    }
   ],
   "source": [
    "# Fitting data to MLP\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "CE = nn.fit(X = train_x_norm,\n",
    "           y = encoded_train_y,\n",
    "           learning_rate = LEARNING_RATE,\n",
    "           epochs = EPOCHS,\n",
    "           optimization = OPTIM,\n",
    "           batch_size = BATCH_SIZE,\n",
    "           prop_dropout = DROPOUT_PROB)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"TIME USED TO TRAIN ENTIRE DATASET IN {} EPOCHS:\".format(EPOCHS))\n",
    "time_total = end - start\n",
    "if time_total > 60:\n",
    "    time_min = time_total // 60\n",
    "    time_min = int(time_min)\n",
    "    time_sec = time_total % 60\n",
    "else:\n",
    "    time_min = 0\n",
    "    time_sec = time_total\n",
    "\n",
    "if time_min > 1:\n",
    "    print(\"{} minutes {:.2f} seconds\".format(time_min, time_sec))\n",
    "else:\n",
    "    print(\"{} minute {:.2f} seconds\".format(time_min, time_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c3482fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Training Loss over Epoch\n",
    "\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.title('Base Model\\nCross Entropy Loss over Epoch', fontsize=20) \n",
    "pl.plot(CE, color=\"navy\")\n",
    "pl.ylabel('Loss')\n",
    "pl.xlabel('Epoch')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Training accuracy over Epoch\n",
    "\n",
    "# Decode one-hot encoded y_hat labels back to class labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd4909",
   "metadata": {},
   "source": [
    "# 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c3bfd90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict(x=test_x_norm, prop_dropout=DROPOUT_PROB)\n",
    "\n",
    "print(predictions)\n",
    "    \n",
    "decoded_predictions = np.zeros(predictions.shape[0])\n",
    "for prediction_idx, prediction_vector in enumerate(predictions):\n",
    "    decoded_predictions[prediction_idx] = int(np.argmax(prediction_vector)) \n",
    "\n",
    "print(decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "22bc4858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/qwxzs1_d0qv0lbbz056t7mlc0000gn/T/ipykernel_1231/3960293885.py:13: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  accuracy = np.sum(predictions ==  test_y_norm[:, 0]) / predictions.shape[0]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8381c9",
   "metadata": {},
   "source": [
    "# 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a785a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16dd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87aa27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51005fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
